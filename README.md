# OIP Task 2 — Токенизация и лемматизация

Проект для обработки сохранённых веб-страниц: извлечение текста, токенизация, фильтрация и группировка токенов по леммам.

## Что делает проект

1. **Список URL** — формирует `urls.txt` со ссылками на статьи русской Википедии.
2. **Скачивание страниц** — краулер сохраняет HTML в папку `pages/` и записывает индекс в `index.txt`.
3. **Токенизация и лемматизация** — из HTML извлекается текст, для каждой страницы создаётся папка `page1`, `page2`, … в `tokenized_pages/` с файлами `tokens.txt` и `lemmas.txt`.

## Требования

- Python 3.9+
- Зависимости: `pymorphy2`, словарь `pymorphy2-dicts-ru`

## Установка

```bash
# Клонировать репозиторий и перейти в папку проекта
cd OIP_TASK_2

# Создать виртуальное окружение
python3 -m venv .venv
source .venv/bin/activate   # Linux/macOS
# или: .venv\Scripts\activate   # Windows

# Установить зависимости
pip install -r requirements.txt
```

## Запуск (по шагам)

### Шаг 1. Сформировать список URL

```bash
python get_url_list.py
```

Создаётся файл **urls.txt** с ссылками на статьи русской Википедии (не менее 100).

### Шаг 2. Скачать страницы

```bash
python crawler.py
```

- Скачивает страницы из `urls.txt` в папку **pages/** (файлы `1.txt`, `2.txt`, …).
- Сохраняет **index.txt**: номер файла и URL страницы.
- Скачивается не менее 100 страниц (настраивается в `crawler.py`).

### Шаг 3. Токены и леммы

```bash
python lemma_token_builder.py
```

- Читает HTML из **pages/*.txt**.
- Извлекает текст (без тегов, script, style).
- Токенизация: только русские слова (длина от 2 до 40 символов).
- Фильтрация: без дубликатов, без служебных слов и мусора.
- Группировка по леммам (pymorphy2).

В результате создаётся папка **tokenized_pages/** с подпапками по страницам:

- **tokenized_pages/page1/** — `tokens.txt` (уникальные токены страницы 1), `lemmas.txt` (леммы и токены).
- **tokenized_pages/page2/** — то же для страницы 2.
- … **tokenized_pages/page104/** и т.д.

## Формат выходных файлов

| Файл | Формат строки |
|------|----------------|
| **tokenized_pages/page\<N\>/tokens.txt** | `<токен>` + перевод строки |
| **tokenized_pages/page\<N\>/lemmas.txt** | `<лемма> <токен1> <токен2> ... <токенN>` + перевод строки |