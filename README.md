# OIP Task 2 — Токенизация и лемматизация

Проект для обработки сохранённых веб-страниц: извлечение текста, токенизация, фильтрация и группировка токенов по леммам.

## Что делает проект

1. **Список URL** — формирует `urls.txt` со ссылками на статьи русской Википедии.
2. **Скачивание страниц** — краулер сохраняет HTML в папку `pages/` и записывает индекс в `index.txt`.
3. **Токенизация и лемматизация** — из HTML извлекается текст, строятся списки токенов и лемм с учётом правил задания.

## Требования

- Python 3.9+
- Зависимости: `pymorphy2`, словарь `pymorphy2-dicts-ru`

## Установка

```bash
# Клонировать репозиторий и перейти в папку проекта
cd OIP_TASK_2

# Создать виртуальное окружение (по желанию)
python3 -m venv .venv
source .venv/bin/activate   # Linux/macOS
# или: .venv\Scripts\activate   # Windows

# Установить зависимости
pip install -r requirements.txt
```

## Запуск (по шагам)

### Шаг 1. Сформировать список URL

```bash
python get_url_list.py
```

Создаётся файл **urls.txt** с ссылками на статьи русской Википедии (не менее 100).

### Шаг 2. Скачать страницы

```bash
python crawler.py
```

- Скачивает страницы из `urls.txt` в папку **pages/** (файлы `1.txt`, `2.txt`, …).
- Сохраняет **index.txt**: номер файла и URL страницы.
- Скачивается не менее 100 страниц (настраивается в `crawler.py`).

### Шаг 3. Токены и леммы

```bash
python lemma_token_builder.py
```

- Читает HTML из **pages/*.txt**.
- Извлекает текст (без тегов, script, style).
- Токенизация: только русские слова (длина от 2 до 40 символов).
- Фильтрация
- Группировка по леммам (pymorphy2).

В результате создаются:

- **tokens.txt** — список уникальных токенов (один токен на строку).
- **lemmas.txt** — лемматизированные группы: в каждой строке лемма и через пробел все токены этой леммы.

## Формат выходных файлов

| Файл | Формат строки |
|------|----------------|
| **tokens.txt** | `<токен>` + перевод строки |
| **lemmas.txt** | `<лемма> <токен1> <токен2> ... <токенN>` + перевод строки |